{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import word_tokenize # word tokenizer, used to split up sentence in words\n",
    "from tqdm import tqdm # TQDM Progress Bar (optional, but make sure to remove references in code)\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 dimensional word embeddings detected. Loading word embeddings...\n",
      "A total of 400000 word embeddings were found.\n",
      "Done with word embeddings!\n",
      "\n",
      "Preprocessed data loaded!\n",
      "Balanced dataset size:  212876\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessor Cell for CNN\n",
    "\n",
    "# Set the variable below to `True`  to reload and save processed data to file.\n",
    "# Set the variable below to `False` to load processed data from file.\n",
    "REBUILD_DATA = False\n",
    "\n",
    "class DataPreprocessor():\n",
    "    TRAINING_DATA = \"train.csv\"\n",
    "    COLS_INCLUDE = {\"comment_text\": \"object\", \"target\": \"float64\"}\n",
    "    \n",
    "    # Leave these to 0, they are set automatically:\n",
    "    toxiccnt = 0\n",
    "    nontoxiccnt = 0\n",
    "    \n",
    "    SENTENCE_LENGTH = 50    # This is the length of all sentences in the training_data.\n",
    "                            # Shorter sentences will be padded to this length, while\n",
    "                            # Longer sentences will be truncated\n",
    "    \n",
    "    EMBEDDING_DIM = 50      # Specificy which glove embeddings to use by their dimensionality\n",
    "    \n",
    "    word2embedding = {} # gives you the glove embedding for a word\n",
    "    embedding_dim = 0 # dimension of glove embeddings. this value will be automatically set!\n",
    "    \n",
    "    balanced_dataset = []\n",
    "    training_data = []\n",
    "    \n",
    "    def make_balanced_dataset(self):\n",
    "        print(\"Building balanced dataset...\")\n",
    "        \n",
    "        # Set up which columns to read from csv file\n",
    "        dtypes = self.COLS_INCLUDE\n",
    "        headers = []\n",
    "        for key in dtypes:\n",
    "            headers.append(key)\n",
    "        \n",
    "        # Read csv file\n",
    "        df = pd.read_csv(self.TRAINING_DATA, header=0, usecols=headers, dtype=dtypes)\n",
    "        \n",
    "        # Create 2 DataFrames for toxic and non-toxic comments.\n",
    "        toxic_df = df[df.target > 0.5]\n",
    "        nontoxic_df = df[df.target <= 0.5]\n",
    "        \n",
    "        # Calculate number of toxic and non-toxic comments.\n",
    "        toxiccnt = toxic_df.target.count().astype(int)\n",
    "        nontoxiccnt = nontoxic_df.target.count().astype(int)\n",
    "        totalcnt = toxiccnt + nontoxiccnt\n",
    "        print(\"Dataset Statistics:\")\n",
    "        print(\"  - Number total comments:     \", totalcnt)\n",
    "        print(\"  - Number toxic comments:     \", toxiccnt)\n",
    "        print(\"  - Number non-toxic comments: \", nontoxiccnt)\n",
    "        \n",
    "        # Get the desired number of toxic and nontoxic comments\n",
    "        desired_cnt = min(toxiccnt, nontoxiccnt)\n",
    "        \n",
    "        # Pick 'desired_cnt' number of random toxic and nontoxic comments\n",
    "        print(\"Balancing dataset...\")\n",
    "        toxic_comments = toxic_df.loc[np.random.choice(toxic_df.index.values, desired_cnt)].to_numpy()\n",
    "        nontoxic_comments = nontoxic_df.loc[np.random.choice(nontoxic_df.index.values, desired_cnt)].to_numpy()\n",
    "        \n",
    "        # Delete panda dataframes from memory, we are done with it.\n",
    "        del toxic_df\n",
    "        del nontoxic_df\n",
    "        del df\n",
    "        \n",
    "        # Build balanced dataset (interweave toxic and nontoxic comments)\n",
    "        self.balanced_dataset = [] # clear balanced dataset if not empty\n",
    "        a = toxic_comments.copy()\n",
    "        b = nontoxic_comments.copy()\n",
    "        if (len(a) != len(b)):\n",
    "            # Should never be here since we should've selected an equal number of toxic/non-toxic comments.\n",
    "            print(\"Warning, size of toxic comments does not match size of nontoxic comments\")\n",
    "        c = max(len(a), len(b))\n",
    "        for i in range(c):\n",
    "            if i < len(a):\n",
    "                # Insert a toxic comment to the dataset\n",
    "                self.balanced_dataset.append(a[i])\n",
    "            if i < len(b):\n",
    "                # Insert a non-toxic comment to the dataset\n",
    "                self.balanced_dataset.append(b[i])\n",
    "        \n",
    "        # Print results\n",
    "        print(\"Balanced dataset size: \", len(self.balanced_dataset))\n",
    "        print(\"First 2 comments in balanced dataset (should be alterating b/n toxic and nontoxic):\")\n",
    "        print(self.balanced_dataset[0])\n",
    "        print(self.balanced_dataset[1])\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    def load_glove_embeddings(self, gloveFileName):\n",
    "        with open(gloveFileName, 'r') as glovef:\n",
    "            # Clear word2embedding dictionary if not already empty\n",
    "            self.word2embedding = {}\n",
    "            \n",
    "            # Get the number of dimensions of each word embedding\n",
    "            firstline = glovef.readline().split()\n",
    "            self.embedding_dim = len(firstline)-1\n",
    "            del firstline\n",
    "            glovef.seek(0) # go back to beginning of file\n",
    "            print(self.embedding_dim, \"dimensional word embeddings detected.\",\n",
    "                 \"Loading word embeddings...\")\n",
    "            \n",
    "            # Loop thru every line in the glove file\n",
    "            for line in glovef:\n",
    "                lineArr = line.split()\n",
    "                word = lineArr[0]\n",
    "                embedding = []\n",
    "                # build word embedding\n",
    "                for i in range(1,self.embedding_dim+1):\n",
    "                    embedding.append(float(lineArr[i]))\n",
    "                \n",
    "                # Save word embedding in word2embedding dictionary\n",
    "                self.word2embedding[word] = embedding\n",
    "            print(\"A total of\", len(self.word2embedding), \"word embeddings were found.\")\n",
    "        \n",
    "        # Create an unknown word embedding\n",
    "        self.word2embedding['<UNKNOWN>'] = [0.0] * self.embedding_dim\n",
    "        self.word2embedding['<PAD>'] = [0.0] * self.embedding_dim\n",
    "        print(\"Done with word embeddings!\\n\")\n",
    "    \n",
    "    \n",
    "    def make_training_data(self):\n",
    "        print(\"Building training data (this will take a while)...\")\n",
    "        \n",
    "        self.training_data = [] # clear training_data if not already empty\n",
    "        #self.training_data = np.ndarray(shape=(len(self.balanced_dataset),2), dtype=float)\n",
    "        \n",
    "        unknownWordEmbedding = self.word2embedding['<UNKNOWN>']\n",
    "        padWordEmbedding = self.word2embedding['<PAD>']\n",
    "        # Loop thru each comment and it's toxicity in the balanced dataset\n",
    "        for i in tqdm(range(len(self.balanced_dataset))):\n",
    "            # self.balanced_dataset[i][0] = toxicity level (0 - 1)\n",
    "            # self.balanced_dataset[i][1] = comment (string)\n",
    "            \n",
    "            # Extract words from sentence\n",
    "            sent_words = word_tokenize(self.balanced_dataset[i][1])\n",
    "            for wordi in range(self.SENTENCE_LENGTH):\n",
    "                if wordi < len(sent_words):\n",
    "                    # Convert word to embedding\n",
    "                    sent_words[wordi] = sent_words[wordi].lower()\n",
    "                    sent_words[wordi] = self.word2embedding.get(sent_words[wordi], unknownWordEmbedding)\n",
    "                else:\n",
    "                    # Add padding\n",
    "                    sent_words.append(padWordEmbedding)\n",
    "            \n",
    "            \n",
    "            self.training_data.append([sent_words[:self.SENTENCE_LENGTH], self.balanced_dataset[i][0]])\n",
    "        \n",
    "        #print(\"First item in training data:\")\n",
    "        #print(self.training_data[0])\n",
    "        print(\"Done with training data!\\n\")\n",
    "    \n",
    "    \n",
    "    def save(self):\n",
    "        print(\"Saving preprocessed training_data to file...\")\n",
    "        # Save training_data to file\n",
    "        np.save(\"cnn-preprocessed-training-data\", self.training_data)\n",
    "    \n",
    "    \n",
    "    def load(self):\n",
    "        # Load training_data from file\n",
    "        self.training_data = np.load(\"cnn-preprocessed-training-data.npy\", allow_pickle=True)\n",
    "        print(\"Preprocessed data loaded!\")\n",
    "\n",
    "\n",
    "preprocessor = DataPreprocessor()\n",
    "preprocessor.load_glove_embeddings(\"glove.6B.\"+str(preprocessor.EMBEDDING_DIM)+\"d.txt\")\n",
    "if REBUILD_DATA:\n",
    "    preprocessor.make_balanced_dataset()\n",
    "    preprocessor.make_training_data()\n",
    "    preprocessor.save()\n",
    "    print(\"Preprocessing data complete!\")\n",
    "else:\n",
    "    preprocessor.load()\n",
    "    print(\"Balanced dataset size: \", len(preprocessor.training_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence toxicity:  0.0\n",
      "If a nontoxic sentence could be an image, this is what it would look like:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sentence = np.asarray(preprocessor.training_data[1][0], dtype=np.float64)\n",
    "toxicity = preprocessor.training_data[1][1]\n",
    "print(\"Sentence toxicity: \", toxicity)\n",
    "\n",
    "if toxicity > 0.5:\n",
    "    print(\"If a toxic sentence could be an image, this is what it would look like:\")\n",
    "else:\n",
    "    print(\"If a nontoxic sentence could be an image, this is what it would look like:\")\n",
    "\n",
    "plt.imshow(sentence, cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on the GPU\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "    print(\"running on the GPU\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"running on the CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicityNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1,  32,  5)\n",
    "        self.conv2 = nn.Conv2d(32, 64,  5)\n",
    "        self.conv3 = nn.Conv2d(64, 128, 5)\n",
    "        \n",
    "        x = torch.randn(preprocessor.SENTENCE_LENGTH, preprocessor.embedding_dim)\n",
    "        x = x.view(-1, 1, preprocessor.SENTENCE_LENGTH, preprocessor.embedding_dim)\n",
    "        self._to_linear = None\n",
    "        self.convs(x)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self._to_linear, 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "    \n",
    "    \n",
    "    def convs(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), (2,2))\n",
    "        x = F.max_pool2d(F.relu(self.conv3(x)), (2,2))\n",
    "        \n",
    "        if self._to_linear is None:\n",
    "            self._to_linear = x[0].shape[0]*x[0].shape[1]*x[0].shape[2]\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1, self._to_linear)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "#     def __init__(self):\n",
    "#         super().__init__()\n",
    "#         self.convs = nn.ModuleList([\n",
    "#             nn.Conv2d(1, 1, (5, preprocessor.embedding_dim))\n",
    "#         ])\n",
    "        \n",
    "#         x = torch.randn(preprocessor.SENTENCE_LENGTH, preprocessor.embedding_dim)\n",
    "#         x = x.view(-1, 1, preprocessor.SENTENCE_LENGTH, preprocessor.embedding_dim)\n",
    "#         self._to_linear = None\n",
    "#         self.do_convs(x)\n",
    "        \n",
    "#         self.fc1 = nn.Linear(self._to_linear, 128)\n",
    "#         self.fc2 = nn.Linear(128, 1)\n",
    "    \n",
    "#     def do_convs(self, x):\n",
    "#         xs = []\n",
    "#         for conv in self.convs:\n",
    "#             x2 = F.relu(conv(x))\n",
    "#             x2 = torch.squeeze(x2, -1)\n",
    "#             x2 = F.max_pool1d(x2, x2.size(2))\n",
    "#             xs.append(x2)\n",
    "#         x = torch.cat(xs, 2)\n",
    "        \n",
    "#         if self._to_linear is None:\n",
    "#             self._to_linear = xs[len(xs)-1].shape[0]*xs[len(xs)-1].shape[1]*xs[len(xs)-1].shape[2]\n",
    "        \n",
    "#         return x\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         x = self.do_convs(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "#         x = F.relu(self.fc1(x))\n",
    "#         x = self.fc2(x)\n",
    "#         return x\n",
    "    \n",
    "\n",
    "net = ToxicityNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved!\n"
     ]
    }
   ],
   "source": [
    "MODEL_SAVE_PATH = \"./cnn-toxicity-model-\"+str(preprocessor.SENTENCE_LENGTH)+'x'+str(preprocessor.embedding_dim)+'.pt'\n",
    "\n",
    "# Load Model from file.\n",
    "def loadModel(path):\n",
    "    net.load_state_dict(torch.load(path))\n",
    "    print(\"Model loaded!\")\n",
    "\n",
    "# Save Model to file\n",
    "def saveModel(path):\n",
    "    torch.save(net.state_dict(), path)\n",
    "    print(\"Model saved!\")\n",
    "\n",
    "# Load default\n",
    "#loadModel(MODEL_SAVE_PATH)\n",
    "# Load Model 50x50 with like 5 epochs\n",
    "#loadModel(\"./cnn-toxicity-model-50x50-backup.pt\")\n",
    "\n",
    "# Save default\n",
    "saveModel(MODEL_SAVE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded in Tensors.\n"
     ]
    }
   ],
   "source": [
    "# Tensor with all the comments\n",
    "X = torch.Tensor([i[0] for i in preprocessor.training_data]).view(-1, preprocessor.SENTENCE_LENGTH, preprocessor.embedding_dim)\n",
    "# Tensor with all the comments' toxicity\n",
    "y = torch.Tensor([i[1] for i in preprocessor.training_data])\n",
    "\n",
    "EVAL_PCT = 0.10 # percent of training data to use for evaluation\n",
    "data_size = int(len(X)*EVAL_PCT)\n",
    "\n",
    "# Split up dataset b/n training and evaluation\n",
    "# Training data\n",
    "train_X = X[:-data_size]\n",
    "train_y = y[:-data_size]\n",
    "# Evaluation data\n",
    "eval_X = X[-data_size:]\n",
    "eval_y = y[-data_size:]\n",
    "\n",
    "\n",
    "VALID_PCT = 0.5 # percent of evaluation data to use as validation data\n",
    "test_size = int(len(eval_X)*VALID_PCT)\n",
    "# Testing data\n",
    "test_X = eval_X[-test_size:]\n",
    "test_y = eval_y[-test_size:]\n",
    "# Validation data\n",
    "validation_X = eval_X[:-test_size]\n",
    "validation_y = eval_y[:-test_size]\n",
    "\n",
    "\n",
    "print(\"Data loaded in Tensors.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "loss_function = nn.MSELoss() # Maybe change to to BCELoss?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev_loss = np.Inf\n",
    "prev_precision = 0.0\n",
    "prev_recall = 0.0\n",
    "\n",
    "prev_loss = 0.07\n",
    "prev_precision = 0.85\n",
    "prev_recall = 0.81"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "\n",
    "def train(net):\n",
    "    global prev_loss\n",
    "    global prev_precision\n",
    "    global prev_recall\n",
    "    \n",
    "    for epoch in range(EPOCHS):\n",
    "        for i in range(0, len(train_X), BATCH_SIZE):\n",
    "            batch_X = train_X[i:i+BATCH_SIZE].view(-1,1,preprocessor.SENTENCE_LENGTH, preprocessor.embedding_dim)\n",
    "            batch_y = train_y[i:i+BATCH_SIZE]\n",
    "            \n",
    "            batch_X, batch_y = batch_X.to(device), batch_y.to(device)\n",
    "\n",
    "            net.train()\n",
    "            net.zero_grad()\n",
    "            outputs = net(batch_X).squeeze()\n",
    "            loss = loss_function(outputs, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Evaluate\n",
    "            if i % len(train_X) == 0:\n",
    "                loss, acc, precision, recall = test(net, validation_X, validation_y)\n",
    "                if precision >= prev_precision and recall >= prev_recall and loss <= prev_loss:\n",
    "                    # Save model\n",
    "                    print(\"Saving model\")\n",
    "                    saveModel(MODEL_SAVE_PATH)\n",
    "                    \n",
    "                    # Update previous values\n",
    "                    prev_precision = precision\n",
    "                    prev_recall = recall\n",
    "                    prev_loss = loss\n",
    "            \n",
    "\n",
    "        print(f\"Epoch: {epoch+1}. Loss: {loss}\\n\")\n",
    "\n",
    "def test(net, data, labels, print_results=True):\n",
    "    net.eval()\n",
    "    \n",
    "    losses = []\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    true_pos = 0\n",
    "    true_neg = 0\n",
    "    false_pos = 0\n",
    "    false_neg = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for i in range(len(data)):\n",
    "            real_toxicity = labels[i].to(device)\n",
    "            predicted_toxicity = net(data[i].view(-1,1,preprocessor.SENTENCE_LENGTH, preprocessor.embedding_dim).to(device))[0]\n",
    "            \n",
    "            loss = loss_function(predicted_toxicity.squeeze(), real_toxicity)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            \n",
    "            if real_toxicity > 0.5:\n",
    "                if predicted_toxicity > 0.5:\n",
    "                    true_pos += 1\n",
    "                else:\n",
    "                    false_neg += 1\n",
    "            if real_toxicity <= 0.5:\n",
    "                if predicted_toxicity <= 0.5:\n",
    "                    true_neg += 1\n",
    "                else:\n",
    "                    false_pos += 1\n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    # Calculate average loss\n",
    "    avg_loss = np.mean(losses)\n",
    "    \n",
    "    # Calculate accuracy, precision, recall\n",
    "    if total != 0:\n",
    "        accuracy = (true_pos+true_neg)/total\n",
    "    else:\n",
    "        total = -1.0\n",
    "    if true_pos+false_pos != 0:\n",
    "        precision = true_pos/(true_pos+false_pos)\n",
    "    else:\n",
    "        precision = -1.0\n",
    "    if true_pos+false_neg != 0:\n",
    "        recall = true_pos/(true_pos+false_neg)\n",
    "    else:\n",
    "        recall = -1.0\n",
    "\n",
    "    if print_results:\n",
    "        print(\"Loss: \", round(avg_loss, 5))\n",
    "        print(\"Accuracy:\", round(accuracy, 3))\n",
    "        print(\"Precision:\", round(precision, 3))\n",
    "        print(\"Recall:\", round(recall, 3))\n",
    "    \n",
    "    return (avg_loss, accuracy, precision, recall)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.05476\n",
      "Accuracy: 0.828\n",
      "Precision: 0.886\n",
      "Recall: 0.752\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.054761742183739984,\n",
       " 0.8275862068965517,\n",
       " 0.8857901726427623,\n",
       " 0.7521142642360459)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test(net, test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  0.05739\n",
      "Accuracy: 0.833\n",
      "Precision: 0.883\n",
      "Recall: 0.768\n",
      "Epoch: 1. Loss: 0.008272608742117882\n",
      "\n",
      "Loss:  0.0581\n",
      "Accuracy: 0.834\n",
      "Precision: 0.883\n",
      "Recall: 0.769\n",
      "Epoch: 2. Loss: 0.0069097536616027355\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to convert a sentence to a Tensor that can be fed into the neural network\n",
    "\n",
    "def sentenceToTensor(sentence):\n",
    "    unknownWordEmbedding = preprocessor.word2embedding['<UNKNOWN>']\n",
    "    padWordEmbedding = preprocessor.word2embedding['<PAD>']\n",
    "    # Loop thru each comment and it's toxicity in the balanced \n",
    "    \n",
    "    # Extract words from sentence\n",
    "    sent_words = word_tokenize(sentence)\n",
    "    for wordi in range(preprocessor.SENTENCE_LENGTH):\n",
    "        if wordi < len(sent_words):\n",
    "            # Convert word to embedding\n",
    "            sent_words[wordi] = sent_words[wordi].lower()\n",
    "            sent_words[wordi] = preprocessor.word2embedding.get(sent_words[wordi], unknownWordEmbedding)\n",
    "        else:\n",
    "            # Add padding\n",
    "            sent_words.append(padWordEmbedding)\n",
    "    return torch.Tensor(sent_words[:preprocessor.SENTENCE_LENGTH]).view(-1,1,preprocessor.SENTENCE_LENGTH,preprocessor.embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I hate ice cream.\n",
      "0.17321155965328217 , Not Toxic.\n",
      "\n",
      "\n",
      "I hate pizza.\n",
      "0.34952810406684875 , Not Toxic.\n",
      "\n",
      "\n",
      "I hate you.\n",
      "-0.08513252437114716 , Not Toxic.\n",
      "\n",
      "\n",
      "I love you and you're not a loser.\n",
      "0.263325572013855 , Not Toxic.\n",
      "\n",
      "\n",
      "I love you and you're a loser.\n",
      "1.010485291481018 , TOXIC\n",
      "\n",
      "\n",
      "I love Italians and you're not a loser.\n",
      "0.18674738705158234 , Not Toxic.\n",
      "\n",
      "\n",
      "I hate Italians and you're a loser.\n",
      "0.7630307674407959 , TOXIC\n",
      "\n",
      "\n",
      "I hate italian scumbags.\n",
      "1.004913091659546 , TOXIC\n",
      "\n",
      "\n",
      "Die you liberal\n",
      "0.8510742783546448 , TOXIC\n",
      "\n",
      "\n",
      "I Love everyone.\n",
      "0.07497930526733398 , Not Toxic.\n",
      "\n",
      "\n",
      "Put a gun to your head and pull the trigger, dude.\n",
      "0.6512705087661743 , TOXIC\n",
      "\n",
      "\n",
      "I am gay.\n",
      "0.49201905727386475 , Not Toxic.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Testing Cell, have fun.\n",
    "\n",
    "sentence = []\n",
    "\n",
    "sentence.append(\"I hate ice cream.\")\n",
    "sentence.append(\"I hate pizza.\")\n",
    "sentence.append(\"I hate you.\")\n",
    "\n",
    "sentence.append(\"I love you and you're not a loser.\")\n",
    "sentence.append(\"I love you and you're a loser.\")\n",
    "sentence.append(\"I love Italians and you're not a loser.\")\n",
    "sentence.append(\"I hate Italians and you're a loser.\")\n",
    "\n",
    "sentence.append(\"I hate italian scumbags.\")\n",
    "sentence.append(\"Die you liberal\")\n",
    "sentence.append(\"I Love everyone.\")\n",
    "sentence.append(\"Put a gun to your head and pull the trigger, dude.\")\n",
    "sentence.append(\"I am gay.\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(sentence)):\n",
    "        sentTensor = sentenceToTensor(sentence[i]).to(device)\n",
    "        toxicity = net(sentTensor)\n",
    "        print(sentence[i], )\n",
    "        if (toxicity > 0.5):\n",
    "            print(float(toxicity), \", TOXIC\")\n",
    "        else:\n",
    "            print(float(toxicity), \", Not Toxic.\")\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
