{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jtuxhorn/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display plots directly in the notebook instead of in a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849  0.000000  Thank you!! This would make my life a lot less...   \n",
       "2  59852  0.000000  This is such an urgent design problem; kudos t...   \n",
       "3  59855  0.000000  Is this something I'll be able to install on m...   \n",
       "4  59856  0.893617               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "2  ...        2006  rejected      0    0    0      0         0   \n",
       "3  ...        2006  rejected      0    0    0      0         0   \n",
       "4  ...        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "2              0.0                         0                         4  \n",
       "3              0.0                         0                         4  \n",
       "4              0.0                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# These first few cells are us following along with the project at the following link, just adjusting to our data.\n",
    "# https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
    "\n",
    "# Get a subset so that we don't waste time while just trying to get a basic model to work.\n",
    "train = train[:100]\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()    # Count the occurences of words.\n",
    "\n",
    "# Count the words in our sentences.\n",
    "for index, comment in train.comment_text.iteritems():\n",
    "    train_sentences.append([])\n",
    "    \n",
    "    for word in nltk.word_tokenize(comment):\n",
    "        words.update([word.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'.': 224, 'the': 184, 'to': 178, ',': 167, 'a': 114, 'and': 107, 'i': 92, 'of': 91, 'that': 66, 'is': 62, 'it': 58, 'you': 55, 'in': 55, 'on': 55, '!': 52, 'this': 49, 'for': 48, '?': 43, \"'s\": 42, 'be': 42, 'have': 39, 'are': 38, 'comments': 38, 'we': 37, \"n't\": 31, 'with': 30, 'but': 27, '``': 27, \"''\": 27, 'or': 27, 'do': 25, 'more': 25, 'they': 25, 'their': 23, 'as': 23, 'civil': 23, 'so': 21, 'if': 20, 'not': 18, 'other': 18, 'comment': 17, 'who': 17, 'people': 17, ':': 17, ')': 17, 'like': 16, 'will': 16, 'by': 16, 'one': 16, 'there': 16, \"'re\": 16, 'all': 16, 'up': 15, 'system': 15, '(': 15, 'would': 14, \"'m\": 14, 'my': 13, 'an': 13, 'at': 13, 'them': 13, 'has': 13, 'was': 12, 'what': 12, 'your': 11, ';': 11, \"'ll\": 11, 'out': 11, 'from': 11, 'just': 11, '-': 11, 'want': 10, 'great': 10, 'very': 10, 'no': 10, 'being': 10, 'only': 10, 'new': 10, 'think': 9, 'any': 9, 'than': 9, 'about': 9, 'see': 9, 'could': 9, '...': 9, 'read': 8, 'make': 8, 'when': 8, 'which': 8, 'even': 8, 'here': 8, 'some': 8, 'try': 8, 'been': 8, 'much': 8, 'also': 8, 'thanks': 8, 'free': 8, 'going': 8, 'where': 8, 'go': 8, \"'\": 7, 'really': 7, 'lot': 7, 'get': 7, 'right': 7, 'good': 7, 'now': 7, 'take': 7, 'can': 7, 'how': 7, 'because': 7, 'say': 7, 'too': 7, 'us': 7, 'sites': 7, 'he': 7, 'idea': 6, 'well': 6, 'seems': 6, 'these': 6, 'love': 6, 'commenting': 6, 'time': 6, 'ww': 6, 'does': 6, 'then': 6, \"'ve\": 6, 'our': 6, 'following': 6, 'vote': 6, 'enough': 6, 'years': 6, 'sheriff': 6, 'way': 5, 'public': 5, 'trolls': 5, 'thread': 5, 'working': 5, 'online': 5, 'community': 5, 'discussions': 5, 'yet': 5, 'discussion': 5, 'into': 5, 'portland': 5, 'article': 5, 'users': 5, 'over': 5, 'find': 5, 'next': 5, '3': 5, 'had': 5, 'malheur': 5, 'help': 5, 'many': 5, 'week': 5, 'left': 5, 'yes': 5, 'burns': 5, 'replies': 5, 'support': 5, 'thank': 4, 'less': 4, 'let': 4, 'anyone': 4, 'problem': 4, 'site': 4, 'guys': 4, 'bunch': 4, 'allow': 4, \"'d\": 4, 'thing': 4, 'interesting': 4, 'often': 4, 'wo': 4, 'diversity': 4, 'work': 4, 'nice': 4, 'disqus': 4, 'hope': 4, 'several': 4, 'section': 4, 'need': 4, 'question': 4, 'she': 4, 'long': 4, 'sure': 4, 'echo': 4, 'though': 4, 'backend': 4, 'why': 4, 'doing': 4, 'know': 4, 'without': 4, 'content': 4, 'same': 4, 'few': 4, 'vegan': 4, 'having': 4, 'soon': 4, '#': 4, 'back': 4, 'did': 4, 'folks': 4, 'state': 4, 'refuge': 4, 'before': 4, 'last': 4, 'learn': 4, 'click': 4, 'different': 4, 'might': 4, 'available': 4, 'me': 4, 'bundy': 4, 'tuesday': 4, 'jan.': 4, 'john': 4, 'day': 4, 'come': 4, 'ward': 4, 'cool': 3, 'keep': 3, 'something': 3, 'able': 3, 'ranchers': 3, 'seem': 3, 'heard': 3, 'armed': 3, 'threat': 3, 'assume': 3, 'agree': 3, 'everyone': 3, 'group': 3, 'works': 3, 'resident': 3, 'every': 3, 'active': 3, 'awesome': 3, 'glad': 3, 'local': 3, 'real': 3, 'those': 3, 'must': 3, 'against': 3, 'posts': 3, 'feels': 3, 'communities': 3, 'debate': 3, 'things': 3, 'away': 3, 'actually': 3, 'each': 3, 'randomly': 3, 'better': 3, 'especially': 3, 'already': 3, 'become': 3, 'chamber': 3, 'algorithms': 3, 'two': 3, 'asked': 3, 'abuse': 3, '%': 3, 'improve': 3, 'fear': 3, 'death': 3, 'writing': 3, 'idiots': 3, 'articles': 3, 'off': 3, 'welcome': 3, 'since': 3, 'beer': 3, 'staff': 3, 'wait': 3, 'extra': 3, 'service': 3, 'little': 3, 'wweek': 3, 'never': 3, 'wanted': 3, 'thought': 3, 'seen': 3, 'gluten': 3, 'algorithm': 3, 'pretty': 3, '2': 3, 'may': 3, 'were': 3, 'bought': 3, 'year': 3, 'use': 3, 'david': 3, 'across': 3, 'willamette': 3, 'situation': 3, 'play': 3, 'nami': 3, 'multnomah': 3, 'am': 3, 'view': 3, 'pdx': 3, 'buttons': 3, 'said': 3, 'button': 3, 'ipa': 3, 'supporting': 3, 'life': 2, 'such': 2, 'design': 2, 'taking': 2, 'should': 2, 'story': 2, 'wonder': 2, 'person': 2, 'him': 2, 'step': 2, 'direction': 2, 'ridiculous': 2, 'called': 2, 'terrorists': 2, 'gets': 2, 'dildos': 2, 'happy': 2, 'share': 2, 'themselves': 2, 'basically': 2, 'currently': 2, 'give': 2, 'curious': 2, 'engage': 2, 'couple': 2, 'forward': 2, 'sections': 2, 'news': 2, 'stories': 2, 'potential': 2, 'tools': 2, 'interaction': 2, 'current': 2, 'events': 2, 'town': 2, 'platform': 2, 'hopefully': 2, 'unpopular': 2, 'oh': 2, 'anything': 2, 'hand': 2, 'winning': 2, 'arguments': 2, 'member': 2, 'ago': 2, 'encourage': 2, 'making': 2, 'personal': 2, 'attacks': 2, 'opinions': 2, 'silenced': 2, 'important': 2, 'newspaper': 2, 'results': 2, 'answer': 2, 'city': 2, 'history': 2, 'giving': 2, 'publications': 2, 'peer': 2, 'reviews': 2, 'questions': 2, 'designed': 2, 'assuming': 2, 'beta': 2, 'features': 2, 'itself': 2, 'participate': 2, 'voices': 2, 'due': 2, 'prevent': 2, 'opinion': 2, 'review': 2, 'understand': 2, 'number': 2, 'okay': 2, 'entire': 2, 'blocking': 2, 'later': 2, 'wanting': 2, 'reviewing': 2, 'approach': 2, 'troll': 2, '2016': 2, 'rate': 2, 'menu': 2, 'stuff': 2, 'still': 2, 'ca': 2, 'completely': 2, 'related': 2, 'base': 2, 'further': 2, 'signed': 2, 'enterprise': 2, 'feature': 2, 'either': 2, 'pay': 2, 'volunteer': 2, 'another': 2, 'problems': 2, 'link': 2, 'everything': 2, 'options': 2, 'mention': 2, 'moderation': 2, 'small': 2, 'wing': 2, 'probably': 2, 'boxes': 2, '--': 2, 'set': 2, 'traffic': 2, 'takes': 2, 'ones': 2, 'fair': 2, 'straightforward': 2, 'affordable': 2, 'pursuant': 2, 'tax': 2, 'tried': 2, 'post': 2, 'foucault': 2, 'fan': 2, 'prefer': 2, 'notifications': 2, 'version': 2, 'totally': 2, 'getting': 2, 'after': 2, 'email': 2, 'exchange': 2, 'pleased': 2, 'ticket': 2, 'livestock': 2, 'late': 2, 'denzel': 2, 'friends': 2, '10': 2, 'hear': 2, 'federal': 2, 'send': 2, 'profiles': 2, 'consider': 2, 'software': 2, 'account': 2, 'best': 2, 'mrwhiskers': 2, 'cat': 2, 'enthusiast': 2, 'digest': 2, 'business': 2, 'journalism': 2, 'words': 2, 'metric': 2, 'publish': 2, 'journalist': 2, 'means': 2, 'elsewhere': 2, 'someone': 2, 'chance': 2, 'stick': 2, 'around': 2, '18': 2, 'within': 2, 'connection': 2, 'groups': 2, 'list': 2, 'got': 2, 'least': 2, 'quite': 2, 'writers': 2, 'human': 2, 'personally': 2, 'cammo': 2, 'wearing': 2, 'bad': 2, 'building': 2, 'form': 2, 'code': 2, 'half': 2, 'second': 2, 'clicking': 2, 'part': 2, 'random': 2, 'conforming': 2, 'fast': 2, 'commenters': 2, 'rejected': 2, 'i5guy': 2, 'â€™': 2, 'picture': 2, 'point': 2, 'trashing': 2, 'run': 2, 'whole': 2, 'pictures': 2, 'appearing': 2, 'hit': 2, 'piece': 2, 'dress': 2, 'con': 2, 'fur': 2, 'fact': 2, 'looking': 2, 'loyd': 2, 'center': 2, 'missed': 2, '13': 2, 'members': 2, 'socalled': 2, \"'militia\": 2, 'traveled': 2, 'enlist': 2, 'sherif': 2, 'refused': 2, 'approval': 2, 'dave': 2, 'willing': 2, \"'shame\": 2, 'humiliate': 2, 'trespassers': 2, 'leaving': 2, 'terrorist': 2, 'attempting': 2, 'poison': 2, 'toxic': 2, 'divisive': 2, 'brand': 2, \"'politics\": 2, 'arrested': 2, 'prosecuted': 2, 'rallies': 2, 'lands': 2, 'held': 2, 'eugene': 2, 'cities': 2, '19': 2, 'check': 2, 'supportmalheur': 2, 'lived': 2, 'both': 2, 'rural': 2, '25': 2, 'urban': 2, 'sorry': 2, 'action': 2, 'oregonlive': 2, 'definitely': 2, 'accusations': 2, 'bias': 2, 'uncivil': 2, 'down': 2, 'challenge': 2, 'conventional': 2, 'wisdom': 2, 'always': 2, 'surprised': 2, 'word': 2, 'strip': 2, 'guy': 2, 'effective': 2, \"'would\": 1, 'mother': 1, 'done': 1, 'anxiety-inducing': 1, 'urgent': 1, 'kudos': 1, 'impressive': 1, 'install': 1, 'releasing': 1, 'haha': 1, 'losers': 1, 'ur': 1, 'sh*tty': 1, 'hahahahahahahahhha': 1, 'suck': 1, 'ffffuuuuuuuuuuuuuuu': 1, 'motivated': 1, 'mostly': 1, 'greed': 1, 'animals': 1, 'destroy': 1, 'land': 1, 'show': 1, 'combo': 1, 'expected': 1, 'together': 1, 'wow': 1, 'sounds': 1, 'man': 1, 'yelled': 1, 'shut': 1, 'fuck': 1, 'ever': 1, 'protesters': 1, 'violence': 1, 'makes': 1, 'hour': 1, 'sending': 1, 'mail': 1, 'butâ€¦': 1, 'ending': 1, 'deluded': 1, 'jokes': 1, 'grant': 1, 'legitimacy': 1, 'protestors': 1, 'greedy': 1, 'small-minded': 1, 'somehow': 1, 'mass': 1, 'delusion': 1, 'individuals': 1, 'large': 1, 'belongs': 1, 'select': 1, 'profit': 1, 'refrain': 1, 'desire': 1, 'jump': 1, 'look': 1, 'seeing': 1, 'plays': 1, 'neo': 1, 'hall': 1, 'sorts': 1, 'reasons': 1, 'rely': 1, 'reddit': 1, 'sense': 1, 'lacking': 1, 'hectic': 1, 'tempted': 1, 'silence': 1, 'stances': 1, 'angry': 1, 'misogynists': 1, 'racists': 1, '150': 1, 'iq': 1, 'slant': 1, 'diode': 1, 'again': 1, 'considered': 1, 'offensive': 1, 'language': 1, 'facts': 1, 'cogent': 1, 'linear': 1, 'math': 1, 'verboten': 1, 'attempts': 1, 'betterâ€”it': 1, 'innovation': 1, 'ended': 1, 'launch': 1, 'nearly': 1, 'decade': 1, 'purpose': 1, 'introducing': 1, 'limit': 1, 'flow': 1, 'encouraging': 1, 'witch': 1, 'hunts': 1, 'true': 1, 'spam': 1, 'experience': 1, 'chasing': 1, 'rabbits': 1, 'draw': 1, 'primary': 1, 'subject': 1, 'healthy': 1, 'component': 1, 'society': 1, 'degenerate': 1, 'insults': 1, 'distraction': 1, 'limits': 1, 'ability': 1, 'talk': 1, 'believing': 1, 'intention': 1, 'movement': 1, 'commentary': 1, 'chosen': 1, 'reviewed': 1, 'bet': 1, 'thorough': 1, 'major': 1, 'improvement': 1, 'council': 1, 'citizens': 1, 'voice': 1, 'solver': 1, 'needs': 1, 'places': 1, 'serve': 1, 'amazing': 1, 'biasedâ€”i': 1, 'co-founder': 1, 'worked': 1, 'hard': 1, 'combining': 1, 'clever': 1, 'separate': 1, '*good*': 1, '*will*': 1, 'addition': 1, 'meta-analysis': 1, '100': 1, 'perfect': 1, 'months': 1, 'testing': 1, 'solid': 1, 'start': 1, 'christa': 1, 'adding': 1, 'overall': 1, 'upvotes': 1, 'notification': 1, 'settings': 1, 'aim': 1, 'opposite': 1, 'spirited': 1, 'harassment': 1, 'threats': 1, 'respectful': 1, 'debateâ€”we': 1, 'positive': 1, 'speak': 1, 'mind': 1, 'unpopularâ€”so': 1, 'treat': 1, 'respect': 1, 'â€”': 1, 'civility': 1, 'dynamic': 1, 'applaud': 1, 'efforts': 1, 'create': 1, 'technology': 1, 'field': 1, 'hoping': 1, 'thoughtful': 1, 'moving': 1, 'bother': 1, 'devoid': 1, 'hardly': 1, 'excuse': 1, 'fred': 1, 'armisen': 1, 'recorded': 1, 'radio': 1, 'grand': 1, 'theft': 1, 'auto': 1, 'iv': 1, 'call': 1, 'muslims': 1, 'acts': 1, 'pilloried': 1, 'smear': 1, 'religion': 1, 'bash': 1, 'christian': 1, 'sects': 1, 'upvoting': 1, 'publisher': 1, 'turn': 1, 'ton': 1, 'worksâ€”following': 1, 'bookmarking': 1, 'during': 1, 'process': 1, 'myself': 1, 'conversations': 1, 'enthusiasm': 1, 'melinda': 1, 'patience': 1, 'brand-new': 1, 'continue': 1, 'suggestions': 1, 'feedback': 1, 'bitch': 1, 'nuts': 1, 'book': 1, 'woman': 1, 'troll-in-training': 1, 'sake': 1, 'concept': 1, 'plan': 1, 'monetize': 1, 'operation': 1, 'pity': 1, 'lost': 1, 'food': 1, 'mash': 1, 'tun': 1, 'favorite': 1, 'bar': 1, 'delicious': 1, 'tempeh': 1, 'excited': 1, 'beers': 1, 'dozens': 1, 'type': 1, 'voting': 1, 'insane-': 1, '*actually': 1, 'read*': 1, 'multiple': 1, 'non': 1, 'installed': 1, 'oregonlive.com': 1, 'visitors': 1, 'established': 1, 'activity': 1, 'discourage': 1, 'growth': 1, 'project': 1, 'shot': 1, 'luck': 1, 'functionality': 1, 'obviously': 1, 'huge': 1, 'increase': 1, 'visitor': 1, 'count': 1, 'significantly': 1, 'moderator': 1, 'undertake': 1, 'developed': 1, 'trust': 1, 'behalf': 1, 'basis': 1, 'barack': 1, 'obama': 1, 'liberal': 1, 'media': 1, 'conspiracy': 1, 'control': 1, 'police': 1, 'dident': 1, 'spend': 1, '30': 1, 'mixing': 1, 'concrete': 1, 'america': 1, 'fall': 1, 'commies': 1, 'taxes': 1, 'high': 1, 'cant': 1, 'afford': 1, 'keyboard': 1, 'caps': 1, 'lock': 1, 'downhill': 1, 'tea': 1, 'party': 1, 'comeback': 1, 'whats': 1, 'flouraide': 1, 'water': 1, 'supply': 1, 'kind': 1, 'mentioning': 1, 'interested': 1, 'mentioned': 1, 'crazy': 1, 'illustration': 1, 'pitch': 1, 'yellow': 1, 'orange': 1, 'wmcelha': 1, 'exciting': 1, 'buffalo': 1, 'tostones': 1, 'fun': 1, 'soy': 1, 'option': 1, 'killer': 1, 'veggie': 1, 'burger': 1, 'tacos': 1, 'salads': 1, 'polluted': 1, 'far': 1, 'lack': 1, 'gave': 1, 'loud': 1, 'megaphone': 1, 'consistently': 1, 'waste': 1, 'funds': 1, 'trendy': 1, 'projects': 1, 'green': 1, 'bike': 1, 'cost': 1, 'fortune': 1, 'repainted': 1, 'oy': 1, 'slow': 1, 'until': 1, 'picks': 1, 'drive': 1, 'cars': 1, 'cause': 1, 'wear': 1, 'tear': 1, 'roads': 1, 'housing': 1, 'built': 1, 'immediately': 1, 'becomes': 1, 'brilliant': 1, 'three': 1, 'own': 1, 'lead': 1, 'designer': 1, 'big': 1, 'panopticomments': 1, 'suspect': 1, 'ubiquity': 1, 'inadvertently': 1, 'given': 1, '2.0': 1, '1': 1, 'win': 1, 'argument': 1, 'trouble': 1, 'brief': 1, 'appears': 1, 'successful': 1, 'goal': 1, 'extremely': 1, 'taxed': 1, 'claim': 1, 'latter': 1, 'suppose': 1, 'theoretical': 1, 'lucky': 1, 'oregonian': 1, 'decide': 1, 'whether': 1, 'worth': 1, '$': 1, '90m': 1, 'stuck': 1, 'hated': 1, 'californian': 1, 'moniker': 1, 'letter': 1, 'campaign': 1, 'started': 1, \"'70\": 1, 'phoned': 1, 'nancy': 1, 'home': 1, '1979': 1, 'along': 1, 'hired': 1, 'bodyguard': 1, 'protect': 1, 'n': 1, '&': 1, 'd': 1, 'weekend': 1, 'hammond': 1, 'ejection': 1, 'ferguson': 1, 'diamond': 1, 'dance': 1, 'jim': 1, 'd.': 1, 'days': 1, 'lots': 1, 'wishing': 1, 'witty': 1, 'quips': 1, 'bundycon': 1, 'occupation': 1, 'hq': 1, 'sacred': 1, 'cows': 1, 'trough': 1, 'easy': 1, 'amazon': 1, 'western': 1, 'abuses': 1, 'mormons': 1, 'complicated': 1, 'relationship': 1, 'law': 1, 'stat': 1, 'loosely': 1, 'announced': 1, 'demonstration': 1, 'wondering': 1, 'sign': 1, 'uses': 1, 'profile': 1, 'partner': 1, 'sign-up': 1, 'smoother': 1, 'overtaken': 1, 'first': 1, 'its': 1, 'intent': 1, 'engagement': 1, 'rather': 1, 'troll-driven': 1, 'driver': 1, 'intended': 1, 'boost': 1, 'ad': 1, 'rates': 1, 'progress': 1, 'helping': 1, 'test': 1, 'over-arching': 1, 'manage': 1, 'course': 1, 'information': 1, 'decided': 1, 'present': 1, 'yourself': 1, 'differently': 1, 'example': 1, 'portland_hipster15': 1, 'licenses': 1, 'tiered': 1, 'plans': 1, 'certainly': 1, 'stretch': 1, 'imagination': 1, 'chooses': 1, 'brace': 1, 'belden': 1, 'fascinating': 1, 'reported': 1, 'identify': 1, 'customers': 1, 'economics': 1, 'chicken': 1, 'farmers': 1, 'livelihoods': 1, 'squeezed': 1, 'drives': 1, 'alienated': 1, 'economic': 1, 'input': 1, 'user': 1, 'future': 1, 'versions': 1, 'architect': 1, 'aja': 1, 'organization': 1, 'mentally': 1, 'ill': 1, 'mental': 1, 'distress': 1, 'services': 1, 'most': 1, 'beneficial': 1, 'program': 1, 'walk-in': 1, 'matter': 1, 'diagnosis': 1, 'coping': 1, 'skills': 1, 'develop': 1, 'cope': 1, 'county': 1, 'http': 1, '//nami.multnomah.org/': 1, 'difficulty': 1, 'times': 1, 'locations': 1, 'info': 1, 'noticed': 1, 'reply': 1, 'mail.app': 1, 'os': 1, 'x': 1, 'looks': 1, 'came': 1, 'through': 1, 'x-webdoc': 1, '//': 1, 'deal': 1, 'scheme': 1, 'bring': 1, 'once': 1, 'tend': 1, 'blockers': 1, 'moderate': 1, 'adopted': 1, 'stop': 1, 'comprehensive': 1, 'goody': 1, 'researched': 1, 'guide': 1, 'ryan': 1, 'responsible': 1, 'sold': 1, 'bill': 1, 'goods': 1, 'his': 1, 'intellectually': 1, 'domineering': 1, 'brother': 1, 'idolizes': 1, 'ammon': 1, 'similarly': 1, 'situated': 1, 'nutters': 1, 'knows': 1, 'unless': 1, 'live': 1, 'cares': 1, 'bearded': 1, 'narrowly': 1, 'educated': 1, 'nuisances': 1, '.are': 1, 'reason': 1, 'worse': 1, 'constitutional': 1, 'pagent': 1, 'downtown': 1, 'repulsive': 1, 'costumes': 1, 'cabela': 1, 'andy': 1, 'bax': 1, 'brandishing': 1, 'weapons': 1, 'immature': 1, 'male': 1, 'exhibitionism': 1, '.cowboys': 1, 'indians': 1, 'middle': 1, 'aged': 1, 'men': 1, 'framed': 1, 'bandwidth': 1, 'usage': 1, 'browsers..': 1, 'amazingly': 1, 'annoying': 1, 'dozen': 1, 'percentage': 1, '1/2': 1, 'response': 1, 'digress': 1, 'ah': 1, 'end': 1, 'automatic': 1, 'rejection': 1, 'assumed': 1, 'quickly': 1, 'unwilling': 1, 'ensure': 1, 'screen': 1, 'grab': 1, 'hours': 1, 'don': 1, 't': 1, 'energy': 1, 'ask': 1, 'll': 1, 'sorta': 1, 'detected': 1, 'ratings': 1, 'peers': 1, 'reading': 1, 'zero': 1, 'snapped': 1, 'writes': 1, 'background': 1, 'class': 1, 'kigurmi': 1, 'sazac': 1, 'european': 1, 'company': 1, 'checked': 1, 'onzie': 1, 'cheetah': 1, 'target': 1, 'fine': 1, 'suddenly': 1, 'appropiating': 1, 'country': 1, 'furries': 1, 'maybe': 1, 'antagonizing': 1, 'teenagers': 1, 'trying': 1, 'christmas': 1, 'tradition': 1, 'santa': 1, 'known': 1, 'issue': 1, 'installing': 1, 'hey': 1, 'changing': 1, 'four': 1, 'shows': 1, 'providing': 1, 'heavy': 1, 'vibers': 1, 'feelings': 1, 'california': 1, 'using': 1, 'weasel': 1, 'wording': 1, 'referring': 1, 'vanity': 1, 'spread': 1, 'misinformed': 1, 'furriers': 1, 'obsessed': 1, '15': 1, 'furs': 1, 'costume': 1, 'ratio': 1, 'costumers': 1, 'cosplay': 1, 'kumoricon': 1, 'dare': 1, 'family': 1, 'friendly': 1, 'safe': 1, 'interest': 1, 'including': 1, 'art': 1, 'role': 1, 'literature': 1, 'january': 1, '2-3': 1, 'furry': 1, 'meet': 1, 'reporter': 1, 'easily': 1, 'drink': 1, 'item': 1, 'grocery': 1, 'store': 1, 'cooler': 1, 'agrees': 1, 'politically': 1, 'mean': 1, 'civilly': 1, 'kay': 1, 'bars': 1, 'neighborhood': 1, 'hawthorne': 1, 'yelp': 1, 'float': 1, 'seriously': 1, 'lds': 1, 'leadership': 1, 'expressed': 1, 'denial': 1, 'seditious': 1, 'illegal': 1, 'actions': 1, 'cult': 1, 'clan': 1, 'minions': 1, 'strange': 1, 'spot': 1, 'nowhere': 1, 'near': 1, 'busy': 1, 'forcing': 1, 'monitor': 1, 'overkill': 1, 'outsiders': 1, 'handling': 1, 'confusion': 1, 'deleted': 1, 'despite': 1, 'repeated': 1, 'insistence': 1, 'handled': 1, 'in-house': 1, 'political': 1, 'desirability': 1, 'tone': 1, 'actual': 1, 'mods': 1, 'reinforced': 1, 'volunteers': 1, '.vary': 1, 'polite': 1, 'saw': 1, '@': 1, 'christa_m': 1, 'below': 1, 'combats': 1, 'effect': 1, 'contentious': 1, 'issues': 1, 'imagine': 1, 'mainstream': 1, 'viewed': 1, 'inherently': 1, 'trump': 1, 'position': 1, 'white': 1, 'kleins': 1, 'sweet': 1, 'cakes': 1, 'melissa': 1, 'saga': 1, 'convince': 1, 'degree': 1, 'derived': 1, 'prejudice': 1, 'deserving': 1, 'rid': 1, 'messages': 1, 'offered': 1, 'crust': 1, 'impressed': 1, 'dismissive': 1, 'papa': 1, 'murphy': 1, 'street': 1, 'pizzicato': 1, 'hillsdale': 1, 'gf': 1, 'room': 1, 'varying': 1, 'backgrounds': 1, 'honestly': 1, 'scam': 1, 'allowing': 1, 'disagree': 1, 'thru': 1, 'denied': 1, 'case': 1, 'somewhat': 1, 'self-correcting': 1, 'bored': 1, 'speculating': 1, 'however': 1, 'tell': 1, 'although': 1, 'shame': 1, 'resolution': 1, 'files': 1, 'lettering': 1, 'tough': 1, 'lower': 1, '2000s': 1, 'third': 1, 'grew': 1, 'watching': 1, 'kevin': 1, 'smith': 1, 'movies': 1, 'slowing': 1, 'order': 1, 'reflect': 1, 'upon': 1, 'others': 1, 'draws': 1, 'attention': 1, 'humanity': 1, 'recipient': 1, 'approve': 1, 'hi': 1, 'possibilities': 1, 'game': 1, 'behind': 1, 'scenes': 1, 'algorithmically': 1, 'commenter-facing': 1, 'app': 1, 'checks': 1, 'balances': 1, 'coordinated': 1, 'becoming': 1, 'chambers': 1, 'banning': 1, 'insulting': 1, 'unfortunately': 1, 'smarter': 1, 'filters': 1, 'tries': 1, 'context': 1, 'backup': 1, 'low-volume': 1, 'situations': 1, 'plus': 1, 'reviewers': 1, 'remove': 1, 'entirely': 1, 'pushing': 1, 'updates': 1, '*i*': 1, 'development': 1, 'commission': 1, 'kate': 1, 'brown': 1, 'oregon': 1, 'direct': 1, 'correct': 1, 'gap': 1, 'participation': 1, 'elevate': 1, 'five': 1, 'startup': 1, 'scene': 1, 'apart': 1, 'rod': 1, 'mr_whiskers': 1, 'mrwhiskers1': 1, 'ta': 1, 'act': 1, 'usernames': 1, 'place': 1, 'dilly-dally': 1})\n"
     ]
    }
   ],
   "source": [
    "print(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize our comments.\n",
    "train.comment_text = train[\"comment_text\"].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>...</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[This, is, so, cool, ., It, 's, like, ,, 'woul...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[Thank, you, !, !, This, would, make, my, life...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[This, is, such, an, urgent, design, problem, ...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>[Is, this, something, I, 'll, be, able, to, in...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.893617</td>\n",
       "      <td>[haha, you, guys, are, a, bunch, of, losers, .]</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.87234</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id    target                                       comment_text  \\\n",
       "0  59848  0.000000  [This, is, so, cool, ., It, 's, like, ,, 'woul...   \n",
       "1  59849  0.000000  [Thank, you, !, !, This, would, make, my, life...   \n",
       "2  59852  0.000000  [This, is, such, an, urgent, design, problem, ...   \n",
       "3  59855  0.000000  [Is, this, something, I, 'll, be, able, to, in...   \n",
       "4  59856  0.893617    [haha, you, guys, are, a, bunch, of, losers, .]   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack   insult  threat  asian  atheist  \\\n",
       "0         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "1         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "2         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "3         0.000000      0.0         0.000000  0.00000     0.0    NaN      NaN   \n",
       "4         0.021277      0.0         0.021277  0.87234     0.0    0.0      0.0   \n",
       "\n",
       "   ...  article_id    rating  funny  wow  sad  likes  disagree  \\\n",
       "0  ...        2006  rejected      0    0    0      0         0   \n",
       "1  ...        2006  rejected      0    0    0      0         0   \n",
       "2  ...        2006  rejected      0    0    0      0         0   \n",
       "3  ...        2006  rejected      0    0    0      0         0   \n",
       "4  ...        2006  rejected      0    0    0      1         0   \n",
       "\n",
       "   sexual_explicit  identity_annotator_count  toxicity_annotator_count  \n",
       "0              0.0                         0                         4  \n",
       "1              0.0                         0                         4  \n",
       "2              0.0                         0                         4  \n",
       "3              0.0                         0                         4  \n",
       "4              0.0                         4                        47  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We now have a tokenized comment_text column in dataframe.\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "tensor([[-1.3852, -0.9624, -1.0003],\n",
      "        [-1.3570, -1.1749, -0.8354],\n",
      "        [-1.3794, -1.2678, -0.7618],\n",
      "        [-1.3699, -1.1893, -0.8177],\n",
      "        [-1.3667, -1.2508, -0.7792]])\n",
      "Print after training:  tensor([[-0.0341, -4.1614, -4.0227],\n",
      "        [-4.5262, -0.0158, -5.3256],\n",
      "        [-3.7815, -4.2559, -0.0377],\n",
      "        [-0.0226, -4.4558, -4.5316],\n",
      "        [-4.6019, -0.0128, -5.9298]])\n"
     ]
    }
   ],
   "source": [
    "# This is just the tutorial from https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "# Turn each word into an index and put it in a tensor.\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "# Four our data: \"Sentence\" \"Offensive scores\" \n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "# They are not using pretrained embeddings. I don't think we should have to do this.\n",
    "# But it might be easier since it seems like Glove and Word2Vec are easiest to use with libraries\n",
    "# which are not included on Datahub.\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "# Every word is assigned a one-hot vector representation.\n",
    "print(word_to_ix)\n",
    "\n",
    "# Since our data is just raw scores, we shouldn't have to do this. (?)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        # Dimension of the hidden states.\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # A lookup table from indices to vectors.\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Embed our sentence\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # \n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "# Our model in this case has our EMBEDDING_DIM and HIDDEN_DIM as defined above.\n",
    "# Then our vocab size is just however many words we saw in our corpus. Our tagset\n",
    "# is 3, as we're choosing either Noun, Verb, or Det.\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(\"Print after training: \", tag_scores)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
