{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment to install keras\n",
    "# !pip install --user keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "%matplotlib inline\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU,Conv1D,MaxPooling1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D,Bidirectional, GlobalMaxPooling1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import gc\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't display too many rows/cols of DataFrames\n",
    "pd.options.display.max_rows = 15\n",
    "pd.options.display.max_columns = 100\n",
    "\n",
    "# Round decimals when displaying DataFrames\n",
    "pd.set_option('precision', 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>asian</th>\n",
       "      <th>atheist</th>\n",
       "      <th>bisexual</th>\n",
       "      <th>black</th>\n",
       "      <th>buddhist</th>\n",
       "      <th>christian</th>\n",
       "      <th>female</th>\n",
       "      <th>heterosexual</th>\n",
       "      <th>hindu</th>\n",
       "      <th>homosexual_gay_or_lesbian</th>\n",
       "      <th>intellectual_or_learning_disability</th>\n",
       "      <th>jewish</th>\n",
       "      <th>latino</th>\n",
       "      <th>male</th>\n",
       "      <th>muslim</th>\n",
       "      <th>other_disability</th>\n",
       "      <th>other_gender</th>\n",
       "      <th>other_race_or_ethnicity</th>\n",
       "      <th>other_religion</th>\n",
       "      <th>other_sexual_orientation</th>\n",
       "      <th>physical_disability</th>\n",
       "      <th>psychiatric_or_mental_illness</th>\n",
       "      <th>transgender</th>\n",
       "      <th>white</th>\n",
       "      <th>created_date</th>\n",
       "      <th>publication_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>article_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>funny</th>\n",
       "      <th>wow</th>\n",
       "      <th>sad</th>\n",
       "      <th>likes</th>\n",
       "      <th>disagree</th>\n",
       "      <th>sexual_explicit</th>\n",
       "      <th>identity_annotator_count</th>\n",
       "      <th>toxicity_annotator_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>59848</td>\n",
       "      <td>0.00</td>\n",
       "      <td>This is so cool. It's like, 'would you want yo...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:41.987077+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>59849</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Thank you!! This would make my life a lot less...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:42.870083+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>59852</td>\n",
       "      <td>0.00</td>\n",
       "      <td>This is such an urgent design problem; kudos t...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:45.222647+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>59855</td>\n",
       "      <td>0.00</td>\n",
       "      <td>Is this something I'll be able to install on m...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2015-09-29 10:50:47.601894+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>59856</td>\n",
       "      <td>0.89</td>\n",
       "      <td>haha you guys are a bunch of losers.</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.87</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015-09-29 10:50:48.488476+00</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2006</td>\n",
       "      <td>rejected</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  target                                       comment_text  \\\n",
       "0  59848    0.00  This is so cool. It's like, 'would you want yo...   \n",
       "1  59849    0.00  Thank you!! This would make my life a lot less...   \n",
       "2  59852    0.00  This is such an urgent design problem; kudos t...   \n",
       "3  59855    0.00  Is this something I'll be able to install on m...   \n",
       "4  59856    0.89               haha you guys are a bunch of losers.   \n",
       "\n",
       "   severe_toxicity  obscene  identity_attack  insult  threat  asian  atheist  \\\n",
       "0             0.00      0.0             0.00    0.00     0.0    NaN      NaN   \n",
       "1             0.00      0.0             0.00    0.00     0.0    NaN      NaN   \n",
       "2             0.00      0.0             0.00    0.00     0.0    NaN      NaN   \n",
       "3             0.00      0.0             0.00    0.00     0.0    NaN      NaN   \n",
       "4             0.02      0.0             0.02    0.87     0.0    0.0      0.0   \n",
       "\n",
       "   bisexual  black  buddhist  christian  female  heterosexual  hindu  \\\n",
       "0       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "1       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "2       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "3       NaN    NaN       NaN        NaN     NaN           NaN    NaN   \n",
       "4       0.0    0.0       0.0        0.0     0.0           0.0    0.0   \n",
       "\n",
       "   homosexual_gay_or_lesbian  intellectual_or_learning_disability  jewish  \\\n",
       "0                        NaN                                  NaN     NaN   \n",
       "1                        NaN                                  NaN     NaN   \n",
       "2                        NaN                                  NaN     NaN   \n",
       "3                        NaN                                  NaN     NaN   \n",
       "4                        0.0                                 0.25     0.0   \n",
       "\n",
       "   latino  male  muslim  other_disability  other_gender  \\\n",
       "0     NaN   NaN     NaN               NaN           NaN   \n",
       "1     NaN   NaN     NaN               NaN           NaN   \n",
       "2     NaN   NaN     NaN               NaN           NaN   \n",
       "3     NaN   NaN     NaN               NaN           NaN   \n",
       "4     0.0   0.0     0.0               0.0           0.0   \n",
       "\n",
       "   other_race_or_ethnicity  other_religion  other_sexual_orientation  \\\n",
       "0                      NaN             NaN                       NaN   \n",
       "1                      NaN             NaN                       NaN   \n",
       "2                      NaN             NaN                       NaN   \n",
       "3                      NaN             NaN                       NaN   \n",
       "4                      0.0             0.0                       0.0   \n",
       "\n",
       "   physical_disability  psychiatric_or_mental_illness  transgender  white  \\\n",
       "0                  NaN                            NaN          NaN    NaN   \n",
       "1                  NaN                            NaN          NaN    NaN   \n",
       "2                  NaN                            NaN          NaN    NaN   \n",
       "3                  NaN                            NaN          NaN    NaN   \n",
       "4                  0.0                            0.0          0.0    0.0   \n",
       "\n",
       "                    created_date  publication_id  parent_id  article_id  \\\n",
       "0  2015-09-29 10:50:41.987077+00               2        NaN        2006   \n",
       "1  2015-09-29 10:50:42.870083+00               2        NaN        2006   \n",
       "2  2015-09-29 10:50:45.222647+00               2        NaN        2006   \n",
       "3  2015-09-29 10:50:47.601894+00               2        NaN        2006   \n",
       "4  2015-09-29 10:50:48.488476+00               2        NaN        2006   \n",
       "\n",
       "     rating  funny  wow  sad  likes  disagree  sexual_explicit  \\\n",
       "0  rejected      0    0    0      0         0              0.0   \n",
       "1  rejected      0    0    0      0         0              0.0   \n",
       "2  rejected      0    0    0      0         0              0.0   \n",
       "3  rejected      0    0    0      0         0              0.0   \n",
       "4  rejected      0    0    0      1         0              0.0   \n",
       "\n",
       "   identity_annotator_count  toxicity_annotator_count  \n",
       "0                         0                         4  \n",
       "1                         0                         4  \n",
       "2                         0                         4  \n",
       "3                         0                         4  \n",
       "4                         4                        47  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('./train.csv')\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into training and test set:\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train, train[[\"severe_toxicity\", \"obscene\", \"identity_attack\", \"insult\", \"threat\"]], test_size = 0.10, random_state = 42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(train.copy()[:200000], train.copy()[:200000][[\"severe_toxicity\", \"obscene\", \"identity_attack\", \"insult\", \"threat\"]], test_size = 0.10, random_state = 69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store the comments as seperate variables for further processing.\n",
    "list_sentences_train = X_train[\"comment_text\"]\n",
    "list_sentences_test = X_test[\"comment_text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_features = 20000\n",
    "tokenizer = Tokenizer(num_words=max_features,char_level=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(list(list_sentences_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "list_sentences_test = tokenizer.texts_to_sequences(list_sentences_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# list_tokenized_train[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "208.0"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find mean length of the sentences\n",
    "length_sentences = [len(comment) for comment in list_tokenized_train]\n",
    "import statistics\n",
    "statistics.median(length_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set length of all sentences to 200 characters. Pad zeros for sentences with length < 200\n",
    "maxlen = 200\n",
    "X_t = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "X_te = pad_sequences(list_sentences_test, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2, 11,  2,  8,  8,  1,  3, 10,  4,  3,  1, 16,  9,  2, 19,  2,\n",
       "         9,  1,  3,  5,  1, 22,  2,  1,  3, 10,  4,  3,  1, 20,  4, 17,\n",
       "        21,  1,  1,  5,  7,  1,  3, 10,  2,  1,  5,  3, 10,  2,  9,  1,\n",
       "        10,  4,  7, 12, 25,  1,  3, 10,  2,  9,  2,  1,  4,  9,  2,  1,\n",
       "        10,  5, 15,  2, 11,  2,  8,  8,  1, 19,  4, 15,  6, 11,  6,  2,\n",
       "         8,  1,  3, 10,  4,  3,  1,  4,  9,  2,  1,  3,  9, 17,  6,  7,\n",
       "        18,  1, 23,  2,  9, 17,  1, 10,  4,  9, 12,  1,  7,  5,  3,  1,\n",
       "         3,  5,  1, 22,  2,  1, 10,  5, 15,  2, 11,  2,  8,  8, 21,  1,\n",
       "         1, 16,  2,  9, 10,  4, 16,  8,  1, 20,  2,  1,  7,  2,  2, 12,\n",
       "         1,  8,  5, 15,  2,  1,  7,  2, 20,  1,  3,  2,  9, 15,  6,  7,\n",
       "         5, 11,  5, 18, 17,  1,  3,  5,  1, 12,  6,  8,  3,  6,  7, 18,\n",
       "        13,  6,  8, 10,  1, 22,  2,  3, 20,  2,  2,  7,  1,  3, 10,  2,\n",
       "         1, 18,  9,  5, 13, 16,  8, 21]], dtype=int32)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_t[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "def load_embedding_matrix():\n",
    "    embeddings_index = dict(get_coefs(*o.strip().split()) for o in open(f'./glove.6B.50d.txt'))  \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = 50\n",
    "    word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CNN_model():\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    \n",
    "#     x = Embedding(len(tokenizer.word_index)+1, embed_size)(inp)\n",
    "    embedding_matrix = load_embedding_matrix()\n",
    "    x = Embedding(len(tokenizer.word_index), embedding_matrix.shape[1],weights=[embedding_matrix],trainable=False)(inp)\n",
    "\n",
    "    x = Conv1D(filters=100,kernel_size=4,padding='same', activation='relu')(x)\n",
    "    \n",
    "    x=MaxPooling1D(pool_size=4)(x)\n",
    "\n",
    "    x = Bidirectional(GRU(60, return_sequences=True,name='lstm_layer',dropout=0.2,recurrent_dropout=0.2))(x)\n",
    "\n",
    "    x = GlobalMaxPooling1D()(x)\n",
    "\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(5, activation=\"sigmoid\")(x)\n",
    "\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                      optimizer='adam',\n",
    "                     metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_8 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_8 (Embedding)      (None, 200, 50)           22500     \n",
      "_________________________________________________________________\n",
      "conv1d_4 (Conv1D)            (None, 200, 100)          20100     \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 50, 100)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 50, 120)           57960     \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_4 (Glob (None, 120)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                6050      \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 255       \n",
      "=================================================================\n",
      "Total params: 106,865\n",
      "Trainable params: 84,365\n",
      "Non-trainable params: 22,500\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = CNN_model()\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from tensorflow.python.client import device_lib\n",
    "# from keras import backend as K\n",
    "# K.tensorflow_backend._get_available_gpus()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 180000 samples, validate on 20000 samples\n",
      "Epoch 1/2\n",
      "180000/180000 [==============================] - 752s 4ms/step - loss: 0.1169 - accuracy: 0.8910 - val_loss: 0.0994 - val_accuracy: 0.8916\n",
      "Epoch 2/2\n",
      "180000/180000 [==============================] - 745s 4ms/step - loss: 0.0993 - accuracy: 0.8918 - val_loss: 0.0932 - val_accuracy: 0.8915\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_t,y_train, batch_size=batch_size, epochs=epochs,validation_data=(X_te,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('./test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_sentences_test_data = test_data[\"comment_text\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_tokenized_test_data = tokenizer.texts_to_sequences(list_sentences_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_data = pad_sequences(list_tokenized_test_data, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97320/97320 [==============================] - 125s 1ms/step\n"
     ]
    }
   ],
   "source": [
    "y_test_data = model.predict(X_test_data,batch_size=batch_size,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(y_test_data, columns = [\"severe_toxicity\", \"obscene\", \"identity_attack\", \"insult\", \"threat\"]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([test_data.reset_index(drop=True),df_results.reset_index(drop=True)], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>53679</th>\n",
       "      <td>7150999</td>\n",
       "      <td>I've never understood why NCR takes the lazy, secularist path and generates so many \"hyphenated\" articles, with racial, sexist tones. \\n\\nThey seem bent on division, continuing to hold on to dated categories.\\n\\nIntermarriage happens.  \\n\\nOUR PRESIDENT ISN'T BLACK. He's half black and half white.  He had a fully white mother! \\n\\nBut my pointing out this actual reality will be commented on as racist!!</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94755</th>\n",
       "      <td>7192075</td>\n",
       "      <td>White supremacists hated the black guy, and it was the black guy's fault.  I guess if the uppity negro had minded his place you and your ilk would have been quite a bit happier, right?</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80332</th>\n",
       "      <td>7177652</td>\n",
       "      <td>We're talking about black on black and white on white crime here, genius. And the rate for white on white crime is higher than black on black crime. Why do you get so butthurt about simple facts?</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.27</td>\n",
       "      <td>0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>7103328</td>\n",
       "      <td>Damn with English like that you should be a slave ...no one would pay you .</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.58</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40879</th>\n",
       "      <td>7138199</td>\n",
       "      <td>Bigots huh,  tell me something,  by stating the obvious that most crime in Alaska or in fact the US in general is committed by blacks or other non whites,  by stating the obvious you liberals like to hand a name like bigots or racist on any one that disagrees with them,  shameful</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.13</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.38</td>\n",
       "      <td>0.07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5740</th>\n",
       "      <td>7103060</td>\n",
       "      <td>If you noticed, he said both sides were to blame.  How does that equate to support for Neo-nazism and violent racism?   Do you support blacks killing cops and other white people?</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46567</th>\n",
       "      <td>7143887</td>\n",
       "      <td>Damn lucky the mother didn't die.  Lock this one up and throw away the key.</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.30</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57358</th>\n",
       "      <td>7154678</td>\n",
       "      <td>damn right Richard, up against the wall MOFO's, up against the wall!</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.56</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91248</th>\n",
       "      <td>7188568</td>\n",
       "      <td>What if he looked white and was still one half black would that have made any difference? What's with this white/black stuff that you preach? Why don't you grow up and start using your head?</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41179</th>\n",
       "      <td>7138499</td>\n",
       "      <td>Damn right</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.67</td>\n",
       "      <td>0.05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            id  \\\n",
       "53679  7150999   \n",
       "94755  7192075   \n",
       "80332  7177652   \n",
       "6008   7103328   \n",
       "40879  7138199   \n",
       "5740   7103060   \n",
       "46567  7143887   \n",
       "57358  7154678   \n",
       "91248  7188568   \n",
       "41179  7138499   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                comment_text  \\\n",
       "53679  I've never understood why NCR takes the lazy, secularist path and generates so many \"hyphenated\" articles, with racial, sexist tones. \\n\\nThey seem bent on division, continuing to hold on to dated categories.\\n\\nIntermarriage happens.  \\n\\nOUR PRESIDENT ISN'T BLACK. He's half black and half white.  He had a fully white mother! \\n\\nBut my pointing out this actual reality will be commented on as racist!!   \n",
       "94755  White supremacists hated the black guy, and it was the black guy's fault.  I guess if the uppity negro had minded his place you and your ilk would have been quite a bit happier, right?                                                                                                                                                                                                                                \n",
       "80332  We're talking about black on black and white on white crime here, genius. And the rate for white on white crime is higher than black on black crime. Why do you get so butthurt about simple facts?                                                                                                                                                                                                                     \n",
       "6008   Damn with English like that you should be a slave ...no one would pay you .                                                                                                                                                                                                                                                                                                                                             \n",
       "40879  Bigots huh,  tell me something,  by stating the obvious that most crime in Alaska or in fact the US in general is committed by blacks or other non whites,  by stating the obvious you liberals like to hand a name like bigots or racist on any one that disagrees with them,  shameful                                                                                                                                \n",
       "5740   If you noticed, he said both sides were to blame.  How does that equate to support for Neo-nazism and violent racism?   Do you support blacks killing cops and other white people?                                                                                                                                                                                                                                      \n",
       "46567  Damn lucky the mother didn't die.  Lock this one up and throw away the key.                                                                                                                                                                                                                                                                                                                                             \n",
       "57358  damn right Richard, up against the wall MOFO's, up against the wall!                                                                                                                                                                                                                                                                                                                                                    \n",
       "91248  What if he looked white and was still one half black would that have made any difference? What's with this white/black stuff that you preach? Why don't you grow up and start using your head?                                                                                                                                                                                                                          \n",
       "41179  Damn right                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "       severe_toxicity  obscene  identity_attack  insult  threat  \n",
       "53679  0.09             0.14     0.21             0.37    0.08    \n",
       "94755  0.08             0.16     0.20             0.34    0.08    \n",
       "80332  0.08             0.13     0.19             0.27    0.09    \n",
       "6008   0.08             0.29     0.13             0.58    0.06    \n",
       "40879  0.08             0.13     0.19             0.38    0.07    \n",
       "5740   0.08             0.16     0.18             0.36    0.08    \n",
       "46567  0.08             0.30     0.11             0.63    0.06    \n",
       "57358  0.08             0.29     0.14             0.56    0.06    \n",
       "91248  0.08             0.16     0.18             0.32    0.08    \n",
       "41179  0.07             0.29     0.09             0.67    0.05    "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top10 = df.sort_values(by='severe_toxicity', ascending=False).head(10)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "top10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # serialize model to JSON\n",
    "# model_json = model.to_json()\n",
    "# with open(\"model.json\", \"w\") as json_file:\n",
    "#     json_file.write(model_json)\n",
    "# # serialize weights to HDF5\n",
    "# model.save_weights(\"model.h5\")\n",
    "# print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load json and create model\n",
    "# json_file = open('model.json', 'r')\n",
    "# loaded_model_json = json_file.read()\n",
    "# json_file.close()\n",
    "# loaded_model = model_from_json(loaded_model_json)\n",
    "# # load weights into new model\n",
    "# loaded_model.load_weights(\"model.h5\")\n",
    "# print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
