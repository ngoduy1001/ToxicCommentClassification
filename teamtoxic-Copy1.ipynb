{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display plots directly in the notebook instead of in a new window\n",
    "%matplotlib inline\n",
    "\n",
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wholeTrainingSet = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first few cells are us following along with the project at the following link, just adjusting to our data.\n",
    "# https://blog.floydhub.com/long-short-term-memory-from-zero-to-hero-with-pytorch/\n",
    "\n",
    "# note: Once model is working, we should use the actual test set here.\n",
    "# Get a subset so that we don't waste time while just trying to get a basic model to work.\n",
    "test_set  = wholeTrainingSet.copy()[10001:18001]\n",
    "training_set = wholeTrainingSet.copy()[:10000]\n",
    "\n",
    "# Training comments.\n",
    "train_comments = training_set.comment_text.values\n",
    "\n",
    "# Split test comments into two sets. One for validation, one for testing after training.\n",
    "split_frac = .5\n",
    "split_id = int(split_frac * len(test_set))\n",
    "validation_set, test_set= test_set[:split_id], test_set[split_id:]\n",
    "\n",
    "test_comments = test_set.comment_text.values\n",
    "validation_comments = validation_set.comment_text.values\n",
    "\n",
    "print(len(test_comments))\n",
    "print(len(train_comments))\n",
    "print(len(validation_comments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test_comments\n",
      "[list(['<<they', 'are', 'working', 'together>>', 'lol'])\n",
      " list(['\"', 'individuals', 'with', 'pre-existing', 'conditions', 'didn', \"'\", 't', 'pay', 'the', 'same.', '\"', 'wrong.', 'that', 'is', 'what', 'happens', 'in', 'a', 'real', 'insurance', 'system', ',', 'because', 'insurance', 'is', 'based', 'on', 'risk', 'to', 'the', 'company.', 'with', 'obamacare', 'there', 'is', 'no', 'risk', 'to', 'the', 'insurance', 'companies', 'because', 'they', 'are', 'acting', 'as', 'nothing', 'more', 'than', 'a', 'government', 'agency', 'with', 'endless', 'capital.', 'the', 'only', 'other', 'option', 'they', 'will', 'have', 'is', 'rationing', 'their', 'services.', 'obamacare', 'will', 'either', 'bankrupt', 'the', 'country', 'or', 'we', 'will', 'be', 'saddled', 'with', 'a', 'healthcare', 'system', 'that', 'has', 'to', 'pick', 'who', 'and', 'when', 'the', 'individual', 'gets', 'treated', '.'])]\n",
      "CPU times: user 17.5 s, sys: 190 ms, total: 17.7 s\n",
      "Wall time: 17.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#thank you Duy\n",
    "toktok = nltk.ToktokTokenizer()\n",
    "test_comments = np.array([toktok.tokenize(sent.lower()) for sent in test_comments])\n",
    "\n",
    "print(\"Processed test_comments\")    \n",
    "\n",
    "print(test_comments[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed train_comments\n",
      "[list(['this', 'is', 'so', 'cool.', 'it', \"'\", 's', 'like', ',', \"'\", 'would', 'you', 'want', 'your', 'mother', 'to', 'read', 'this??', \"'\", 'really', 'great', 'idea', ',', 'well', 'done', '!'])\n",
      " list(['thank', 'you', '!', '!', 'this', 'would', 'make', 'my', 'life', 'a', 'lot', 'less', 'anxiety-inducing.', 'keep', 'it', 'up', ',', 'and', 'don', \"'\", 't', 'let', 'anyone', 'get', 'in', 'your', 'way', '!'])]\n",
      "CPU times: user 2min 25s, sys: 3.29 s, total: 2min 28s\n",
      "Wall time: 2min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "train_comments = np.array([toktok.tokenize(sent.lower()) for sent in train_comments])\n",
    "    \n",
    "print(\"Processed train_comments\")\n",
    "print(train_comments[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed validation_comments\n",
      "[list(['i', \"'\", 'd', 'prefer', 'my', 'child', 'to', 'be', 'using', 'pot', 'over', 'alcohol', 'or', 'cigarettes', 'any', 'day', 'of', 'the', 'week.', 'rather', 'than', 'passing', 'legislation', 'to', 'do', 'this', 'or', 'that', ',', 'why', 'not', 'just', 'de-list', 'it', ',', 'making', 'it', 'of', 'about', 'the', 'same', 'stature', 'as', 'dandelions', '?', 'by', 'doing', 'so', 'the', 'it', 'becomes', 'common', 'place', 'and', 'even', 'boring', ',', 'the', 'already', 'downward', 'trend', 'in', 'adolescent', 'use', 'would', 'probably', 'accelerate', '.'])\n",
      " list(['perhaps', 'just', 'a', 'war', 'on', 'real', 'estate', 'agents', ',', 'brokers', 'and', 'realtors', 'that', 'have', 'won', 'big', 'time', 'with', 'current', 'fee', 'structure.', 'found', 'much', 'more', 'value', 'in', 'real', 'estate', 'lawyer', 'for', '$', '2k', 'than', '$', '50k+', 'for', 'agent', 'fees', '.'])]\n"
     ]
    }
   ],
   "source": [
    "validation_comments = np.array([toktok.tokenize(sent.lower()) for sent in validation_comments])\n",
    "\n",
    "print(\"Processed validation_comments\")\n",
    "print(validation_comments[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = Counter()\n",
    "\n",
    "# Count the words in our tokenized training set sentences.\n",
    "for sentence in train_comments:\n",
    "    for word in sentence:\n",
    "        words.update([word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "492197\n",
      "205618\n"
     ]
    }
   ],
   "source": [
    "# Remove words that only appear once.\n",
    "\n",
    "print(len(words))\n",
    "for key in list(words):\n",
    "    if(words[key] == 1):\n",
    "        del words[key]\n",
    "        \n",
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort words by frequency and add \"_UNKNOWN\" to our list\n",
    "words = sorted(words, key=words.get, reverse=True)\n",
    "words = [\"_UNKNOWN\"] + words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to map words to their index\n",
    "word2idx = {o:i for i,o in enumerate(words)}\n",
    "\n",
    "# A dictionary to map indexes to their word.\n",
    "idx2word = {i:o for i,o in enumerate(words)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 11, 1, 204, 19]\n"
     ]
    }
   ],
   "source": [
    "# Convert tokenized sentence so that its words are replaced by the indices of its word from the word2idx dictionary.\n",
    "def convertTokenizedSentenceToIDX(sentence):\n",
    "    for i in range(len(sentence)):\n",
    "        if sentence[i] in word2idx:\n",
    "            # If it's in word2idx, it has an assigned index.\n",
    "            sentence[i] = word2idx[sentence[i]]\n",
    "        else:\n",
    "            # If it's not in the word2idx, then it's unknown.\n",
    "            sentence[i] = 0\n",
    "    \n",
    "    return sentence\n",
    "            \n",
    "\n",
    "print(convertTokenizedSentenceToIDX(['Is', 'that', 'the', 'law', '?']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed test_comments\n",
      "Processed train_comments\n",
      "Processed validation_comments\n"
     ]
    }
   ],
   "source": [
    "# Convert all our words to their indices.\n",
    "\n",
    "for i in range(len(test_comments)):\n",
    "    test_comments[i] = convertTokenizedSentenceToIDX(test_comments[i])\n",
    "\n",
    "print(\"Processed test_comments\")    \n",
    "\n",
    "for i in range(len(train_comments)):\n",
    "    train_comments[i] = convertTokenizedSentenceToIDX(train_comments[i])\n",
    "    \n",
    "print(\"Processed train_comments\")    \n",
    "\n",
    "for i in range(len(validation_comments)):\n",
    "    validation_comments[i] = convertTokenizedSentenceToIDX(validation_comments[i])\n",
    "\n",
    "print(\"Processed validation_comments\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "[list([27, 8, 45, 12305, 14, 7, 17, 61, 2, 7, 50, 15, 118, 43, 1106, 3, 205, 31464, 7, 138, 182, 385, 2, 120, 248, 34])\n",
      " list([384, 15, 34, 34, 27, 50, 101, 69, 221, 6, 236, 243, 0, 189, 14, 70, 2, 4, 67, 7, 24, 159, 218, 71, 9, 43, 114, 34])\n",
      " list([27, 8, 163, 55, 10554, 3226, 212, 142, 4456, 3, 15, 16, 412, 14, 1035, 111, 5723, 34])\n",
      " list([8, 27, 184, 13, 7, 214, 21, 390, 3, 6486, 23, 69, 1194, 19, 65, 37, 15, 21, 6317, 14, 19])\n",
      " list([5997, 15, 778, 18, 6, 1086, 5, 2987, 10])]\n",
      "Testing:\n",
      "[list([82219, 18, 341, 0, 740])\n",
      " list([12, 1133, 26, 8177, 1713, 174, 7, 24, 147, 1, 2661, 12, 1146, 11, 8, 36, 850, 9, 6, 180, 544, 239, 2, 83, 544, 8, 343, 23, 826, 3, 1, 4956, 26, 1537, 54, 8, 47, 826, 3, 1, 544, 448, 83, 25, 18, 1895, 28, 157, 56, 74, 6, 116, 2068, 26, 3119, 11859, 1, 81, 84, 1587, 25, 37, 22, 8, 21522, 39, 3285, 1537, 37, 342, 3173, 1, 172, 31, 30, 37, 21, 14738, 26, 6, 1019, 239, 11, 49, 3, 1102, 42, 4, 65, 1, 849, 423, 1617, 10])\n",
      " list([13, 7, 179, 248, 11, 933, 32, 13, 993, 59, 185, 16, 187, 332, 184, 16594, 36, 25, 22, 59, 10677, 4, 13, 606, 25, 92, 193, 10])\n",
      " list([3529, 67, 7, 24, 516, 43, 2317, 637, 5552, 34])\n",
      " list([68, 102, 11, 151, 66, 9, 2752, 19])]\n",
      "Validation:\n",
      "[list([13, 7, 249, 1460, 69, 525, 3, 21, 379, 1174, 99, 1396, 31, 6688, 80, 235, 5, 1, 3691, 316, 74, 2297, 1399, 3, 51, 27, 31, 11, 2, 89, 20, 59, 185730, 14, 2, 286, 14, 5, 52, 1, 132, 16853, 28, 49120, 19, 40, 228, 45, 1, 14, 1559, 562, 297, 4, 92, 4804, 2, 1, 250, 8519, 3642, 9, 14148, 169, 50, 315, 13161, 10])\n",
      " list([323, 59, 6, 350, 23, 180, 1240, 3378, 2, 10451, 4, 9612, 11, 22, 241, 213, 88, 26, 396, 2499, 10739, 479, 104, 56, 722, 9, 180, 1240, 1985, 16, 109, 21804, 74, 109, 98764, 16, 3440, 1606, 10])\n",
      " list([4060, 34, 248, 154, 61, 9728, 117, 53, 9, 34554, 19, 1215, 64, 7568, 19, 2387, 117, 3184, 29618, 19, 68, 104, 124, 49, 1, 44782, 0, 113515, 353, 3, 12884, 4300, 19, 13, 630, 8057, 388, 6016, 2, 37908, 4, 532, 1824, 5717, 18, 38, 11, 339, 3, 77, 10])\n",
      " list([705, 2, 11, 7, 17, 1, 2611, 9939, 359, 58700, 2888, 29, 250, 110713, 9, 27, 172, 10])\n",
      " list([144, 82, 110, 402, 619, 29, 6, 322, 13811, 54, 29, 6, 1350, 1538, 5, 653, 16, 1, 2615, 845, 20625, 3, 1, 266, 880, 3, 6, 104, 1350, 18137, 13, 1441, 2730, 9, 1, 292, 7200, 29, 81, 550, 3, 100, 6, 1554, 7798, 145, 330, 9, 87, 991, 2, 32, 104, 243, 74, 9, 1, 376, 10])]\n"
     ]
    }
   ],
   "source": [
    "# We now have a vectorized comment array for all three sets.\n",
    "print(\"Training:\")\n",
    "print(train_comments[:5])\n",
    "\n",
    "print(\"Testing:\")\n",
    "print(test_comments[:5])\n",
    "\n",
    "print(\"Validation:\")\n",
    "print(validation_comments[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad all the sentences so that they are 200 in length. This has the words at the beginning followed by 0's. I'm not\n",
    "# sure if that matters or not.\n",
    "\n",
    "def padSentence(sentence):\n",
    "    padded = np.zeros(200, dtype=int)\n",
    "    \n",
    "    for i in range(len(sentence)):\n",
    "        if i >= 200:\n",
    "            break\n",
    "            \n",
    "        padded[i] = sentence[i]\n",
    "    \n",
    "    return padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pad the sentences to a fixed length\n",
    "\n",
    "for i in range(len(test_comments)):\n",
    "    test_comments[i] = padSentence(test_comments[i])\n",
    "    \n",
    "\n",
    "for i in range(len(train_comments)):\n",
    "    train_comments[i] = padSentence(train_comments[i])\n",
    "    \n",
    "for i in range(len(validation_comments)):\n",
    "    validation_comments[i] = padSentence(validation_comments[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training:\n",
      "[array([   27,     8,    45, 12305,    14,     7,    17,    61,     2,\n",
      "           7,    50,    15,   118,    43,  1106,     3,   205, 31464,\n",
      "           7,   138,   182,   385,     2,   120,   248,    34,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])]\n",
      "Testing:\n",
      "[array([82219,    18,   341,     0,   740,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "           0,     0])]\n",
      "Validation:\n",
      "[array([    13,      7,    249,   1460,     69,    525,      3,     21,\n",
      "          379,   1174,     99,   1396,     31,   6688,     80,    235,\n",
      "            5,      1,   3691,    316,     74,   2297,   1399,      3,\n",
      "           51,     27,     31,     11,      2,     89,     20,     59,\n",
      "       185730,     14,      2,    286,     14,      5,     52,      1,\n",
      "          132,  16853,     28,  49120,     19,     40,    228,     45,\n",
      "            1,     14,   1559,    562,    297,      4,     92,   4804,\n",
      "            2,      1,    250,   8519,   3642,      9,  14148,    169,\n",
      "           50,    315,  13161,     10,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0,\n",
      "            0,      0,      0,      0,      0,      0,      0,      0])]\n"
     ]
    }
   ],
   "source": [
    "# We now have a vectorized comment array for all three sets with a fixed length of 200.\n",
    "\n",
    "print(\"Training:\")\n",
    "print(train_comments[:1])\n",
    "\n",
    "print(\"Testing:\")\n",
    "print(test_comments[:1])\n",
    "\n",
    "print(\"Validation:\")\n",
    "print(validation_comments[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the datatype of our pd.arrays is int instead of object.\n",
    "\n",
    "train_comments = np.vstack(train_comments).astype(int)\n",
    "validation_comments = np.vstack(validation_comments).astype(int)\n",
    "test_comments = np.vstack(test_comments).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create our datasets. This just maps each vectorized sentence to it's corresponding \"target\" column.\n",
    "# The target column is adjust so that any values >= .5 are turned into one, and the others 0.\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "\n",
    "# Create training data DataLoader\n",
    "\n",
    "tensor_train_x = torch.from_numpy(train_comments)\n",
    "tensor_train_y = torch.from_numpy(np.where(training_set.target >= .5, 1, 0))\n",
    "\n",
    "train_data = TensorDataset(tensor_train_x, tensor_train_y)\n",
    "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create validation data DataLoader\n",
    "\n",
    "tensor_validation_x = torch.from_numpy(validation_comments)\n",
    "tensor_validation_y = torch.from_numpy(np.where(validation_set.target >= .5, 1, 0))\n",
    "\n",
    "validation_data = TensorDataset(tensor_validation_x, tensor_validation_y)\n",
    "validation_loader = DataLoader(validation_data, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "\n",
    "# Create test data DataLoader\n",
    "\n",
    "tensor_test_x = torch.from_numpy(test_comments)\n",
    "tensor_test_y = torch.from_numpy(np.where(test_set.target >= .5, 1, 0))\n",
    "\n",
    "test_data = TensorDataset(tensor_test_x, tensor_test_y)\n",
    "test_loader = DataLoader(test_data, shuffle=True, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example showing toxic comment #5's representation in our dataset.\n",
    "# The input data is train_comments[4], a vectorized sentence.\n",
    "# The output data is target_vals[4], a 0 or 1.\n",
    "\n",
    "print(training_set.head(6))\n",
    "print(\"Vectorized comment: \", train_comments[4])\n",
    "print(\"Target: \", train_y.T[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_comments))\n",
    "print(len(validation_comments))\n",
    "print(len(test_comments))\n",
    "\n",
    "print()\n",
    "\n",
    "print(len(train_loader))\n",
    "print(len(validation_loader))\n",
    "print(len(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToxicityNet(nn.Module):\n",
    "    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, n_layers, drop_prob=0.5):\n",
    "        super(ToxicityNet, self).__init__()\n",
    "        \n",
    "        # Specify how many outputs our linear layer (output layer) should have.\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # How many times our LSTM is stacked.\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # How many dimenions our hidden state representations should be.\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer.\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Actual LSTM that does all the work.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        # Helps prevent overfitting.\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        # Output layer.\n",
    "        self.fc = nn.Linear(hidden_dim, output_size)\n",
    "        \n",
    "        # Makes sure our outputs are valid probabilities.\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        \n",
    "        # Our batch size is the length of x, as x is just a list of tokenized and indexed sentences.\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # Makes sure the datatype of the word representations is long.\n",
    "        x = x.long()\n",
    "        \n",
    "        # Embed the words.\n",
    "        embeds = self.embedding(x)\n",
    "        \n",
    "        # Run the lstm on the input.\n",
    "        lstm_out, hidden = self.lstm(embeds, hidden)\n",
    "        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n",
    "        \n",
    "        # Dropout (Not really sure what it is, but it helps prevent overfitting.)\n",
    "        out = self.dropout(lstm_out)\n",
    "        \n",
    "        # Linear to produce our target score\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # Makes sure the target score is between 0 and 1\n",
    "        out = self.sigmoid(out)\n",
    "        \n",
    "        # Since out is a list of len(sentence) hidden states, we only return the score corresponding with the last hidden state.\n",
    "        out = out.view(batch_size, -1)\n",
    "        out = out[:,-1]\n",
    "        \n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device),\n",
    "                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().to(device))\n",
    "        return hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = len(word2idx) + 1\n",
    "output_size = 1\n",
    "embedding_dim = 24\n",
    "hidden_dim = 64\n",
    "n_layers = 2\n",
    "\n",
    "model = ToxicityNet(vocab_size, output_size, embedding_dim, hidden_dim, n_layers)\n",
    "model.to(device)\n",
    "\n",
    "lr=0.005\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# How many times to loop over training the dataset.\n",
    "epochs = 30\n",
    "\n",
    "counter = 0\n",
    "print_every = 100\n",
    "clip = 5\n",
    "\n",
    "# Our loss starts at infinity.\n",
    "valid_loss_min = np.Inf\n",
    "\n",
    "# Make sure it's keeping track of our gradients.\n",
    "model.train()\n",
    "for i in range(epochs):\n",
    "    \n",
    "    # Do the default hidden state\n",
    "    h = model.init_hidden(batch_size)\n",
    "    \n",
    "    # For each input, paired with an output label in the training data\n",
    "    for inputs, labels in train_loader:\n",
    "        counter += 1\n",
    "        \n",
    "        # The initial hidden state\n",
    "        h = tuple([e.data for e in h])\n",
    "        \n",
    "        # Send input and label to gpu.\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        # Reset the gradient to zero so the old gradient doesn't interfere with the new gradient.\n",
    "        model.zero_grad()\n",
    "        \n",
    "        # Get the output of our model\n",
    "        output, h = model(inputs, h)\n",
    "        \n",
    "        # Calculate the loss\n",
    "        loss = criterion(output, labels.float())\n",
    "        \n",
    "        # No exploding gradient\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        \n",
    "        # Calculate the gradients\n",
    "        loss.backward()\n",
    "        \n",
    "        # Move the weights toward their optimum\n",
    "        optimizer.step()\n",
    "        \n",
    "        if(counter % print_every == 0):\n",
    "            val_h = model.init_hidden(batch_size)\n",
    "            val_losses = []\n",
    "            \n",
    "            # We are evaluating, so we don't track these operations.\n",
    "            model.eval()\n",
    "            \n",
    "            for val_inp, val_lab in validation_loader:\n",
    "                # Initial hidden state\n",
    "                val_h = tuple([each.data for each in val_h])\n",
    "                \n",
    "                # Send to gpu\n",
    "                val_inp, val_lab = val_inp.to(device), val_lab.to(device)\n",
    "                \n",
    "                # Output of running on validation sentence.\n",
    "                val_out, val_h = model(val_inp, val_h)\n",
    "                \n",
    "                # Get the loss from the loss function.\n",
    "                val_loss = criterion(val_out, val_lab.float())\n",
    "                \n",
    "                # Record the loss on this item.\n",
    "                val_losses.append(val_loss.item())\n",
    "            \n",
    "            # Back to training, track operations.\n",
    "            model.train()\n",
    "            print(\"Epoch: {}/{}...\".format(i+1, epochs),\n",
    "                  \"Step: {}...\".format(counter),\n",
    "                  \"Loss: {:.6f}\".format(np.mean(val_losses)))\n",
    "            \n",
    "            # If these weights improved our performance, then save them.\n",
    "            if(np.mean(val_losses) <= valid_loss_min):\n",
    "                torch.save(model.state_dict(), './state_dict.pt')\n",
    "                print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model ...'.format(valid_loss_min, np.mean(val_losses)))\n",
    "                valid_loss_min = np.mean(val_losses)\n",
    "                    \n",
    "\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the best model\n",
    "model.load_state_dict(torch.load('./state_dict.pt'))\n",
    "\n",
    "test_losses = []\n",
    "num_correct = 0\n",
    "h = model.init_hidden(batch_size)\n",
    "\n",
    "model.eval()\n",
    "for inputs, labels in test_loader:\n",
    "    h = tuple([each.data for each in h])\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    output, h = model(inputs, h)\n",
    "    test_loss = criterion(output.squeeze(), labels.float())\n",
    "    test_losses.append(test_loss.item())\n",
    "    pred = torch.round(output.squeeze())  # Rounds the output to 0/1\n",
    "    correct_tensor = pred.eq(labels.float().view_as(pred))\n",
    "    correct = np.squeeze(correct_tensor.cpu().numpy())\n",
    "    \n",
    "    num_correct += np.sum(correct)\n",
    "\n",
    "print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n",
    "test_acc = num_correct/len(test_loader.dataset)\n",
    "print(\"Test accuracy: {:.3f}%\".format(test_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = model.init_hidden(1)\n",
    "h = tuple([each.data for each in h])\n",
    "\n",
    "x = torch.tensor(convertTokenizedSentenceToIDX(list([\"i\", \"love\", \"you\"])))\n",
    "x = x.view(1,3)\n",
    "x = x.to(device)\n",
    "\n",
    "out, hidden = model(x, h)\n",
    "\n",
    "print(out)\n",
    "\n",
    "if(out[0] >= .5):\n",
    "    print(\"This is probably toxic.\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8}\n",
      "tensor([[-0.7633, -1.4841, -1.1804],\n",
      "        [-0.7582, -1.4659, -1.2018],\n",
      "        [-0.7564, -1.5394, -1.1516],\n",
      "        [-0.8099, -1.4821, -1.1149],\n",
      "        [-0.7384, -1.5232, -1.1903]])\n",
      "Print after training:  tensor([[-0.1197, -2.7209, -3.0572],\n",
      "        [-3.3684, -0.0393, -5.5054],\n",
      "        [-2.7692, -4.7681, -0.0739],\n",
      "        [-0.0474, -3.6342, -3.9161],\n",
      "        [-3.2497, -0.0447, -5.3062]])\n"
     ]
    }
   ],
   "source": [
    "# This is just the tutorial from https://pytorch.org/tutorials/beginner/nlp/sequence_models_tutorial.html\n",
    "\n",
    "# Turn each word into an index and put it in a tensor.\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)\n",
    "\n",
    "\n",
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"])\n",
    "]\n",
    "\n",
    "\n",
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "\n",
    "# Every word is assigned a one-hot vector representation.\n",
    "print(word_to_ix)\n",
    "\n",
    "# Since our data is just raw scores, we shouldn't have to do this. (?)\n",
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}\n",
    "\n",
    "# These will usually be more like 32 or 64 dimensional.\n",
    "# We will keep them small, so we can see how the weights change as we train.\n",
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "\n",
    "class LSTMTagger(nn.Module):\n",
    "    \n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        \n",
    "        # Dimension of the hidden states.\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # A lookup table from indices to vectors.\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        # Embed our sentence\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        \n",
    "        # \n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "# Our model in this case has our EMBEDDING_DIM and HIDDEN_DIM as defined above.\n",
    "# Then our vocab size is just however many words we saw in our corpus. Our tagset\n",
    "# is 3, as we're choosing either Noun, Verb, or Det.\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "# See what the scores are before training\n",
    "# Note that element i,j of the output is the score for tag j for word i.\n",
    "# Here we don't need to train, so the code is wrapped in torch.no_grad()\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    print(tag_scores)\n",
    "\n",
    "for epoch in range(300):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# See what the scores are after training\n",
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!\n",
    "    print(\"Print after training: \", tag_scores)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
